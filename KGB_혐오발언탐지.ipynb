{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\428-3090\\anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from tqdm import trange, tqdm\n",
    "from transformers import AutoModel, ElectraModel, ElectraTokenizer, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "PADDING_TOKEN = 1\n",
    "S_OPEN_TOKEN = 0\n",
    "S_CLOSE_TOKEN = 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device: ', device)\n",
    "\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n",
    "}\n",
    "\n",
    "def jsonload(fname, encoding=\"utf-8\"):\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    return j\n",
    "\n",
    "\n",
    "# json 개체를 파일이름으로 깔끔하게 저장\n",
    "def jsondump(j, fname):\n",
    "    with open(fname, \"w\", encoding=\"UTF8\") as f:\n",
    "        json.dump(j, f, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# jsonl 파일 읽어서 list에 저장\n",
    "def jsonlload(fname, encoding=\"utf-8\"):\n",
    "    json_list = []\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        for line in f.readlines():\n",
    "            json_list.append(json.loads(line))\n",
    "    return json_list\n",
    "\n",
    "# jsonlist를 jsonl 형태로 저장\n",
    "def jsonldump(j_list, fname):\n",
    "    f = open(fname, \"w\", encoding='utf-8')\n",
    "    for json_data in j_list:\n",
    "        f.write(json.dumps(json_data, ensure_ascii=False)+'\\n')\n",
    "\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, args, num_label):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(args.classifier_hidden_size, args.classifier_hidden_size)\n",
    "        self.dropout = nn.Dropout(args.classifier_dropout_prob)\n",
    "        self.output = nn.Linear(args.classifier_hidden_size, num_label)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnethicalExpressionClassifier(nn.Module):\n",
    "    def __init__(self, args, num_label, len_tokenizer):\n",
    "        super(UnethicalExpressionClassifier, self).__init__()\n",
    "\n",
    "        self.num_label = num_label\n",
    "        self.xlm_roberta = AutoModel.from_pretrained(args.base_model)\n",
    "        self.xlm_roberta.resize_token_embeddings(len_tokenizer)\n",
    "\n",
    "        self.labels_classifier = SimpleClassifier(args, self.num_label)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.xlm_roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=None\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.labels_classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_label),\n",
    "                                                labels.view(-1))\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, form, label, max_len):\n",
    "    data_dict = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'label': [],\n",
    "    }\n",
    "    tokenized_data = tokenizer(form, padding='max_length', max_length=max_len, truncation=True)\n",
    "    data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
    "    data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
    "    data_dict['label'].append(label)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def get_dataset(raw_data, tokenizer, max_len):\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    token_labels_list = []\n",
    "\n",
    "    for utterance in raw_data:\n",
    "        tokenized_data = tokenize_and_align_labels(tokenizer, utterance['input'], utterance['output'] , max_len)\n",
    "        input_ids_list.extend(tokenized_data['input_ids'])\n",
    "        attention_mask_list.extend(tokenized_data['attention_mask'])\n",
    "        token_labels_list.extend(tokenized_data['label'])\n",
    "\n",
    "    return TensorDataset(torch.tensor(input_ids_list), torch.tensor(attention_mask_list),\n",
    "                         torch.tensor(token_labels_list))\n",
    "\n",
    "\n",
    "def evaluation(y_true, y_pred):\n",
    "\n",
    "    y_true = list(map(int, y_true))\n",
    "    y_pred = list(map(int, y_pred))\n",
    "\n",
    "    print('f1_score: ', f1_score(y_true, y_pred, average=None))\n",
    "    print('f1_score_micro: ', f1_score(y_true, y_pred, average='micro'))\n",
    "\n",
    "\n",
    "def train_unethical_expression_classifier(args=None):\n",
    "    if not os.path.exists(args.model_path):\n",
    "        os.makedirs(args.model_path)\n",
    "\n",
    "    print('train_unethical_expression_classifier')\n",
    "    print('model would be saved at ', args.model_path)\n",
    "\n",
    "    print('loading train data')\n",
    "    train_data = jsonlload(args.train_data)\n",
    "    dev_data = jsonlload(args.dev_data)\n",
    "\n",
    "    print('tokenizing train data')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.base_model)\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    print('We have added', num_added_toks, 'tokens')\n",
    "    train_dataloader = DataLoader(get_dataset(train_data, tokenizer, args.max_len), shuffle=True,\n",
    "                                  batch_size=args.batch_size)\n",
    "    dev_dataloader = DataLoader(get_dataset(dev_data, tokenizer, args.max_len), shuffle=True,\n",
    "                                batch_size=args.batch_size)\n",
    "\n",
    "    print('loading model')\n",
    "    model = UnethicalExpressionClassifier(args, 2, len(tokenizer))\n",
    "    model.to(device)\n",
    "\n",
    "    # print(model)\n",
    "\n",
    "    FULL_FINETUNING = True\n",
    "    if FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters())\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=args.learning_rate,\n",
    "        eps=args.eps\n",
    "    )\n",
    "    epochs = args.num_train_epochs\n",
    "    max_grad_norm = 1.0\n",
    "    total_steps = epochs * len(train_dataloader)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    epoch_step = 0\n",
    "\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        epoch_step += 1\n",
    "        total_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss, _ = model(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # print('batch_loss: ', loss.item())\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(\"Epoch: \", epoch_step)\n",
    "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "        if args.do_eval:\n",
    "            model.eval()\n",
    "\n",
    "            pred_list = []\n",
    "            label_list = []\n",
    "\n",
    "            for batch in dev_dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    loss, logits = model(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                pred_list.extend(predictions)\n",
    "                label_list.extend(b_labels)\n",
    "\n",
    "            evaluation(label_list, pred_list)\n",
    "\n",
    "        if not os.path.exists(args.model_path):\n",
    "            os.makedirs(args.model_path)\n",
    "\n",
    "        model_saved_path = args.model_path + 'saved_model_epoch_' + str(epoch_step) + '.pt'\n",
    "        torch.save(model.state_dict(), model_saved_path)\n",
    "\n",
    "    print(\"training is done\")\n",
    "\n",
    "\n",
    "def test_unethical_expression_classifier(args):\n",
    "\n",
    "    test_data = jsonlload(args.test_data)\n",
    "    pred_data = jsonlload(args.pred_data)\n",
    "\n",
    "    temp_ground_truth_dict = {}\n",
    "\n",
    "    true_list = []\n",
    "    pred_list = []\n",
    "\n",
    "    # 데이터 list로 변경\n",
    "    for data in test_data:\n",
    "        if data['id'] in temp_ground_truth_dict:\n",
    "            return {\n",
    "                \"error\": \"정답 데이터에 중복된 id를 가지는 경우 존재\"\n",
    "            }\n",
    "        temp_ground_truth_dict[data['id']] = data['output']\n",
    "\n",
    "    for data in pred_data:\n",
    "        if data['id'] not in temp_ground_truth_dict:\n",
    "            return {\n",
    "                \"error\": \"제출 파일과 정답 파일의 id가 일치하지 않음\"\n",
    "            }\n",
    "        true_list.append(temp_ground_truth_dict[data['id']])\n",
    "        pred_list.append(data['output'])\n",
    "\n",
    "    evaluation(true_list, pred_list)\n",
    "\n",
    "\n",
    "def separate_by_s_token(form):\n",
    "    splited_temp_form = form.split('</s></s>')\n",
    "    splited_temp_form[0] = splited_temp_form[0].split('<s>')[-1]\n",
    "    splited_temp_form[-1] = splited_temp_form[-1].split('</s>')[0]\n",
    "\n",
    "    for i in range(len(splited_temp_form)):\n",
    "        splited_temp_form[i] = splited_temp_form[i].strip()\n",
    "\n",
    "    return splited_temp_form\n",
    "\n",
    "\n",
    "def demo_unethical_expression_classifier(args):\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.base_model)\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    test_data = jsonlload(args.test_data)\n",
    "\n",
    "    model = UnethicalExpressionClassifier(args, 2, len(tokenizer))\n",
    "    model.load_state_dict(torch.load(args.model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    for data in tqdm(test_data):\n",
    "        tokenized_data = tokenizer(data['input'], padding='max_length', max_length=args.max_len, truncation=True)\n",
    "\n",
    "        input_ids = torch.tensor([tokenized_data['input_ids']]).to(device)\n",
    "        attention_mask = torch.tensor([tokenized_data['attention_mask']]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, logits = model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        data['output'] = int(predictions[0])\n",
    "\n",
    "    jsonldump(test_data, args.output_dir + 'result.jsonl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    train_data = \"nikluge-au-2022-train.jsonl\"\n",
    "    test_data = \"nikluge-au-2022-test.jsonl\"\n",
    "    pred_data = \"result.jsonl\"\n",
    "    dev_data = \"nikluge-au-2022-dev.jsonl\"\n",
    "    batch_size = 8\n",
    "    learning_rate = 3e-5\n",
    "    eps = 1e-8\n",
    "    do_train = True\n",
    "    do_eval = True\n",
    "    do_test = False\n",
    "    num_train_epochs = 5\n",
    "    base_model = \"lighthouse/mdeberta-v3-base-kor-further\"\n",
    "    model_path = \"saved_models/\"\n",
    "    output_dir = \"output/\"\n",
    "    do_demo = False\n",
    "    max_len = 256\n",
    "    classifier_hidden_size = 768\n",
    "    classifier_dropout_prob = 0.1\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_unethical_expression_classifier\n",
      "model would be saved at  saved_models/\n",
      "loading train data\n",
      "tokenizing train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\428-3090\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 8 tokens\n",
      "loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\428-3090\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Average train loss: 0.3627871389016042\n",
      "f1_score:  [0.93214141 0.90089021]\n",
      "f1_score_micro:  0.9194404245055475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 1/5 [04:30<18:01, 270.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "Average train loss: 0.2199216128258552\n",
      "f1_score:  [0.93695132 0.90365854]\n",
      "f1_score_micro:  0.9237819585142306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 2/5 [09:00<13:31, 270.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n",
      "Average train loss: 0.1298876602609209\n",
      "f1_score:  [0.9276808 0.9      ]\n",
      "f1_score_micro:  0.9160636758321273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 3/5 [13:30<09:00, 270.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n",
      "Average train loss: 0.06597675589429128\n",
      "f1_score:  [0.93261678 0.90561668]\n",
      "f1_score_micro:  0.9213699951760733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 4/5 [18:01<04:30, 270.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n",
      "Average train loss: 0.03117368315082934\n",
      "f1_score:  [0.93895944 0.91260997]\n",
      "f1_score_micro:  0.9281234925229137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [22:28<00:00, 269.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training is done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_unethical_expression_classifier(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    train_data = \"nikluge-au-2022-train.jsonl\"\n",
    "    test_data = \"nikluge-au-2022-test.jsonl\"\n",
    "    pred_data = \"result.jsonl\"\n",
    "    dev_data = \"nikluge-au-2022-dev.jsonl\"\n",
    "    batch_size = 8\n",
    "    learning_rate = 3e-5\n",
    "    eps = 1e-8\n",
    "    do_train = False\n",
    "    do_eval = False\n",
    "    do_test = False\n",
    "    num_train_epochs = 5\n",
    "    base_model = \"lighthouse/mdeberta-v3-base-kor-further\"\n",
    "    model_path = \"saved_models/saved_model_epoch_5.pt\"\n",
    "    output_dir = \"output/\"\n",
    "    do_demo = True\n",
    "    max_len = 256\n",
    "    classifier_hidden_size = 768\n",
    "    classifier_dropout_prob = 0.1\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2072/2072 [00:18<00:00, 110.23it/s]\n"
     ]
    }
   ],
   "source": [
    "demo_unethical_expression_classifier(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_unethical_expression_classifier(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
